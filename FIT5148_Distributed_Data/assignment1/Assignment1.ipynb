{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5148 - Distributed Databases and Big Data\n",
    "\n",
    "# Assignment 1 - Solution Workbook\n",
    "\n",
    "\n",
    "**Instructions:**\n",
    "- You will be using Python 3.\n",
    "- Read the assignment instruction carefully and implement the algorithms in this workbook. \n",
    "- You can use the datasets fireData and climateData (provided below) if you are aiming for Credit Task.\n",
    "- For Distinction and High Distinction tasks, you are required to read the files FireData.csv and ClimateData.CSV provided with the assignment programatically and prepare the data in the correct format so that it can be used in your algorithm. \n",
    "- You can introduce new cells as necessary.\n",
    "\n",
    "**Your details**\n",
    "- Name: Boyu Zhang\n",
    "- Student ID:28491300 \n",
    "\n",
    "- Name:\n",
    "- Student ID:\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import multiprocessing as mp\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "firePath = './data/FireData.csv'\n",
    "climatePath = './data/ClimateData.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_to_list(path):\n",
    "    with open(path,'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        return list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "climateData = read_to_list(climatePath)[1:]\n",
    "fireData = read_to_list(firePath)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " ['948700',\n",
       "  '2016-12-31',\n",
       "  '19',\n",
       "  '56.8',\n",
       "  '7.9',\n",
       "  '11.1',\n",
       "  '   72.0*',\n",
       "  '  61.9*',\n",
       "  ' 0.00I'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a glance on the data of climate\n",
    "climateData == sorted(climateData, key=lambda x : x[1]), climateData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False,\n",
       " [['-37.966',\n",
       "   '145.051',\n",
       "   '341.8',\n",
       "   '2017-12-27T04:16:51',\n",
       "   '26.7',\n",
       "   '78',\n",
       "   '2017-12-27',\n",
       "   '68'],\n",
       "  ['-35.541',\n",
       "   '143.311',\n",
       "   '336.3',\n",
       "   '2017-12-27T00:02:15',\n",
       "   '62',\n",
       "   '82',\n",
       "   '2017-12-27',\n",
       "   '63']])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a glance on the data of fire\n",
    "fireData == sorted(fireData, key=lambda x : x[-1]), fireData[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 Parallel Search\n",
    "#### 1. \n",
    "Write an algorithm to search climate data for the records on ​15th December 2017​. \n",
    "Justify your choice of the data partition technique and search technique you have used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justification**:From the above exploration of the data in the climateData list, we can find out that all data are already sorted in term of the data column which is is excatly our search key.\n",
    "\n",
    "Given this, we can just pick a simplest partition method such as round-robin to partition the dataset evenly which maintains the balance of load without compromising on efficiency.\n",
    "\n",
    "The binary search is obviously the desirable option when the source data is already sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here starts the first half of task1\n",
    "#first pick a partition method:\n",
    "def rr_partition(data,n):\n",
    "    \"\"\"\n",
    "    Perform a simple round robin partition on the given data set\n",
    "    \n",
    "    Parameters:\n",
    "    data: the dataset to be partitioned, which is a list\n",
    "    n: the number of groups that the dataset will be divided into\n",
    "    \n",
    "    Return:\n",
    "    result: the partitioned subset of the dataset \n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in  range(n):\n",
    "        result.append([])\n",
    "    for index,element in enumerate(data):\n",
    "        index_bin = index%n\n",
    "        result[index_bin].append(element)\n",
    "    return result\n",
    "    \n",
    "#then pick a search method:\n",
    "def binary_search(data,key):\n",
    "    \"\"\"\n",
    "    Perform binary search given certain key\n",
    "    \n",
    "    Parameters:\n",
    "    data: the input dataset which is a list\n",
    "    key: an query record\n",
    "    \n",
    "    Return:\n",
    "    found: the mathced record and its position in a tuple, return (-1,None) if not found \n",
    "    \"\"\"\n",
    "    position = -1\n",
    "    found = None\n",
    "    upper = len(data) - 1\n",
    "    lower = 0\n",
    "    \n",
    "    while lower <= upper and not found:\n",
    "        mid = (upper + lower)//2\n",
    "        if data[mid][1] == key:\n",
    "            found = data[mid]\n",
    "            position = mid\n",
    "        elif data[mid][1] < key:\n",
    "            lower = mid + 1\n",
    "        else:\n",
    "            upper = mid - 1     \n",
    "    return found\n",
    "\n",
    "#the complete parrallel search:\n",
    "from multiprocessing import Pool\n",
    "def parallel_search_date(data,query,n_processor):\n",
    "    \"\"\"\n",
    "    A method doing parallel search on a given dataset ,\n",
    "    when given a search clue like a single key or a range for certain column value\n",
    "    \n",
    "    Parameters:\n",
    "    data: the dataset to be searched, which is a list\n",
    "    query: a query record\n",
    "    n_processer: the number of processor to parallize the search job\n",
    "    \n",
    "    Return:\n",
    "    results: the list of all search results in all processors\n",
    "    \"\"\"\n",
    "    results = [read_to_list(climatePath)[0]]\n",
    "    pool = Pool(processes=n_processor)\n",
    "    datasets = rr_partition(data, n_processor)\n",
    "    for partition in datasets:\n",
    "        result = pool.apply_async(binary_search, args=(partition,query))\n",
    "        output = result.get()\n",
    "        results.append(output)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Station',\n",
       "  ' Date',\n",
       "  '   Air Temperature(Celcius)',\n",
       "  '  Relative Humidity',\n",
       "  '  WindSpeed  (knots)',\n",
       "  ' Max Wind Speed',\n",
       "  '   MAX  ',\n",
       "  '  MIN  ',\n",
       "  'Precipitation '],\n",
       " ['948702',\n",
       "  '2017-12-15',\n",
       "  '18',\n",
       "  '52',\n",
       "  '7.1',\n",
       "  '14',\n",
       "  '   74.5*',\n",
       "  '53.1',\n",
       "  ' 0.00I'],\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test the output\n",
    "parallel_search_date(climateData,'2017-12-15',6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.\n",
    "Write an algorithm to find the​ ``latitude``​, ​``longitude`` ​and ​``confidence`` ​when the surface\n",
    "temperature (°C) was between ​65 °C​ and​ 100 °C​. Justify your choice of the data partition\n",
    "technique and search technique you have used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justification**:From the foregoing exploration we can see that the record of fire data is not sorted by surface temperature. THis time the query is a range, it is easy to consider range partition first, however, it appears if the partition range matches the query range, then there is totally no point search in the other partitions which is not a parallized case any more; on the other hand if the two ranges do not match, then there is no point using range patition.\n",
    "\n",
    "All the remaining partition method don't help with optimize the performance of parallel search, thus we still pick the simplest one -- round-robin which has the lowest time complexity and ensure load balance.\n",
    "\n",
    "As for the search method, binary search is not quite compatitable with a range query, which means the mechanism don't reduce search time complexity and can even lead to confusing output.Thus this time we pick just the linear seach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here starts task1 part2\n",
    "def linear_seach(data,key):\n",
    "    \"\"\"\n",
    "    Perform linear search on given dataset\n",
    "    \n",
    "    Parameters:\n",
    "    dat: the dataset to be searched\n",
    "    key: the key(can be a range) used for searching\n",
    "    \n",
    "    Return:\n",
    "    result: a tuple containing the index of the matched record and the query result\n",
    "    \"\"\"\n",
    "    position = -1\n",
    "    found = None\n",
    "    result = []\n",
    "    for record in data:\n",
    "        if int(record[-1]) in range(key[0],key[1]):\n",
    "            found = record[:2] + [record[-3]]\n",
    "            position = data.index(record)\n",
    "            result.append(found)\n",
    "    return result\n",
    "\n",
    "def parallel_search_temperature(data,query,n_processor):\n",
    "    \"\"\"\n",
    "    A method doing parallel search on a given dataset ,\n",
    "    when given a search clue like a single key or a range for certain column value\n",
    "    \n",
    "    Parameters:\n",
    "    data: the dataset to be searched, which is a list\n",
    "    query: a query record\n",
    "    n_processer: the number of processor to parallize the search job\n",
    "    \n",
    "    Return:\n",
    "    results: the list of all search results in all processors\n",
    "    \"\"\"\n",
    "    results = [['Latitude','Longitude','Confidence']]\n",
    "    pool = Pool(processes=n_processor)\n",
    "    datasets = rr_partition(data, n_processor)\n",
    "    for partition in datasets:\n",
    "        result = pool.apply_async(linear_seach, args=(partition,query))\n",
    "        output = result.get()\n",
    "        results += output\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Latitude', 'Longitude', 'Confidence'],\n",
       " ['-37.966', '145.051', '78'],\n",
       " ['-37.875', '142.51', '93'],\n",
       " ['-37.613', '149.305', '95'],\n",
       " ['-37.624', '149.314', '90'],\n",
       " ['-37.95', '142.366', '92'],\n",
       " ['-37.634', '149.237', '100'],\n",
       " ['-37.6', '149.325', '99'],\n",
       " ['-37.609', '149.32', '99'],\n",
       " ['-37.862', '144.175', '87']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test function\n",
    "parallel_search_temperature(fireData,[65,100],2)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3  Parallel Sort\n",
    "Write an algorithm to sort fire data based on surface temperature(°C) in a ascending order. Justify your choice of the data partition technique and sorting technique you have used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justification**:Since each record in the firedata is quite similar, which means the processing time for each of them can be regarded as the same, thus to make best use of all the processors in the parallized process, it is best to consider round-robin as a data partition method which is the one that can maintains best load-balancing as well as smallest complexity in both terms of time and space as a partition method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:5148]",
   "language": "python",
   "name": "conda-env-5148-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
