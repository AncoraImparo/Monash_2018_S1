{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processing for stream data can be : db.collection.insertmany() for `fire`, and db.collection.insertone() for `climate`;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the stream processing function is unable to finish the reference specified in the data model.\n",
    "So we just insert in the collections and later set up the reference before we check the records are already there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can use tupel-slide window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def sendRecord1(record):\n",
    "\n",
    "    word = record[0]\n",
    "    count = record[1]\n",
    "    print(word, count)\n",
    "    \n",
    "    client = MongoClient()\n",
    "    db = client.fit5148_db\n",
    "    collection = db.stream1\n",
    "    collection.update({\"_id\": word}, {\"$inc\": {\"count\": count}}, upsert=True)\n",
    "    client.close()\n",
    "\n",
    "\n",
    "def sendRecord2(record):\n",
    "\n",
    "    word = record[0]\n",
    "    count = record[1]\n",
    "    print(word, count)\n",
    "    \n",
    "    client = MongoClient()\n",
    "    db = client.fit5148_db\n",
    "    collection = db.stream2\n",
    "    collection.update({\"_id\": word}, {\"$inc\": {\"count\": count}}, upsert=True)\n",
    "    client.close()\n",
    "\n",
    "# We add this line to avoid an error : \"Cannot run multiple SparkContexts at once\". If there is an existing spark context, we will reuse it instead of creating a new context.\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# If there is no existing spark context, we now create a new context\n",
    "if (sc is None):\n",
    "    sc = SparkContext()\n",
    "ssc = StreamingContext(sc, 1)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "\n",
    "\n",
    "stream2 = ssc.socketTextStream(host,9999)\n",
    "stream1 = ssc.socketTextStream(host,8080)\n",
    "\n",
    "# Split each line into words\n",
    "words1 = stream1.flatMap(lambda line: line.split(\"\\n\"))\n",
    "words2 = stream2.flatMap(lambda line: line.split(\"\\n\"))\n",
    "# Count each word in each batch\n",
    "pairs1 = words1.map(lambda word: (word, 1))\n",
    "pairs2 = words2.map(lambda word: (word, 1))\n",
    "#wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Output the result                            \n",
    "pairs1.foreachRDD(lambda rdd: rdd.foreach(sendRecord1))\n",
    "pairs2.foreachRDD(lambda rdd: rdd.foreach(sendRecord2))\n",
    "ssc.start()\n",
    "try:\n",
    "    ssc.awaitTermination(timeout=60)\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    sc.stop()\n",
    "\n",
    "ssc.stop()\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:5148]",
   "language": "python",
   "name": "conda-env-5148-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
