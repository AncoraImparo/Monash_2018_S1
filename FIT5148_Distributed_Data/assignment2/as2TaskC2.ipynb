{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processing for stream data can be : db.collection.insertmany() for `fire`, and db.collection.insertone() for `climate`;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems the stream processing function is unable to finish the reference specified in the data model.\n",
    "So we just insert in the collections and later set up the reference before we check the records are already there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can use tupel-slide window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "def sendRecord(record):\n",
    "\n",
    "    word = record[0]\n",
    "    count = record[1]\n",
    "    print(word, count)\n",
    "    \n",
    "    client = MongoClient()\n",
    "    db = client.fit5148_db\n",
    "    collection = db.wc_coll\n",
    "    collection.update({\"_id\": word}, {\"$inc\": {\"count\": count}}, upsert=True)\n",
    "    client.close()\n",
    "    \n",
    "# We add this line to avoid an error : \"Cannot run multiple SparkContexts at once\". If there is an existing spark context, we will reuse it instead of creating a new context.\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# If there is no existing spark context, we now create a new context\n",
    "if (sc is None):\n",
    "    sc = SparkContext(appName=\"WordCountApp\")\n",
    "ssc = StreamingContext(sc, 2)\n",
    "ssc.checkpoint(\"checkpoint\")\n",
    "\n",
    "host = \"localhost\"\n",
    "port = 9999\n",
    "\n",
    "lines = ssc.socketTextStream(host, int(port))\n",
    "\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\",\"))\n",
    "\n",
    "# Count each word in each batch\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Output the result                            \n",
    "wordCounts.foreachRDD(lambda rdd: rdd.foreach(sendRecord))\n",
    "\n",
    "ssc.start()\n",
    "try:\n",
    "    ssc.awaitTermination(timeout=60)\n",
    "except KeyboardInterrupt:\n",
    "    ssc.stop()\n",
    "    sc.stop()\n",
    "\n",
    "ssc.stop()\n",
    "sc.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:5148]",
   "language": "python",
   "name": "conda-env-5148-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
